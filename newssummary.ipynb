{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "newssummary.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "biEEDaXEvHjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import re\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuKkEOvBDM2p",
        "colab_type": "text"
      },
      "source": [
        "Upload the dataset into content/sample_data directory and read from there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImbxNYt7vHjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news = pd.read_csv(\"sample_data/news.csv\", engine = 'python')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAu_PYdmDaim",
        "colab_type": "text"
      },
      "source": [
        "Drop the unnecessary columns from the table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkygvNuBvHj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "news.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzjk4vtSvHj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "c1cdde5e-c4e3-48c3-fb94-1355c92a1b18"
      },
      "source": [
        "news.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4 ex-bank officials booked for cheating bank o...</td>\n",
              "      <td>The CBI on Saturday booked four former officia...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Supreme Court to go paperless in 6 months: CJI</td>\n",
              "      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n",
              "      <td>At least three people were killed, including a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Why has Reliance been barred from trading in f...</td>\n",
              "      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Was stopped from entering my own studio at Tim...</td>\n",
              "      <td>TV news anchor Arnab Goswami has said he was t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline                                              Short\n",
              "0  4 ex-bank officials booked for cheating bank o...  The CBI on Saturday booked four former officia...\n",
              "1     Supreme Court to go paperless in 6 months: CJI  Chief Justice JS Khehar has said the Supreme C...\n",
              "2  At least 3 killed, 30 injured in blast in Sylh...  At least three people were killed, including a...\n",
              "3  Why has Reliance been barred from trading in f...  Mukesh Ambani-led Reliance Industries (RIL) wa...\n",
              "4  Was stopped from entering my own studio at Tim...  TV news anchor Arnab Goswami has said he was t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw0nwAafvHkG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "e88aa99a-d0ca-452a-e31c-2d0226a2b2b2"
      },
      "source": [
        "news.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 55104 entries, 0 to 55103\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Headline  55104 non-null  object\n",
            " 1   Short     55104 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 861.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__tw2JGrDlcs",
        "colab_type": "text"
      },
      "source": [
        "Split the dataset into document and corresponding summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__IkQBeLvHkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document = news['Short']\n",
        "summary = news['Headline']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou7rtiKHDspz",
        "colab_type": "text"
      },
      "source": [
        "Add start and end tokens to summary since the target sequence is unknown for the test sequence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywxk9JpUvHkN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "6ddc90d9-cee1-4600-a7ca-16af22de4998"
      },
      "source": [
        "summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n",
        "summary.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    <go> 4 ex-bank officials booked for cheating b...\n",
              "1    <go> Supreme Court to go paperless in 6 months...\n",
              "2    <go> At least 3 killed, 30 injured in blast in...\n",
              "3    <go> Why has Reliance been barred from trading...\n",
              "4    <go> Was stopped from entering my own studio a...\n",
              "Name: Headline, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBgD4dMiEGGV",
        "colab_type": "text"
      },
      "source": [
        "Filter out the punctuation from the summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYhbxz_OvHkU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
        "oov_token = '<unk>'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-ni-FuZEYyh",
        "colab_type": "text"
      },
      "source": [
        "Create different tokenizers for document and summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbvAfYfLvHkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
        "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJg4EbhLEkxc",
        "colab_type": "text"
      },
      "source": [
        "Fit text to tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb0gUGqavHkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_tokenizer.fit_on_texts(document)\n",
        "summary_tokenizer.fit_on_texts(summary)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsxpak9dvHkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = document_tokenizer.texts_to_sequences(document)\n",
        "targets = summary_tokenizer.texts_to_sequences(summary)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEJQDtE9vHkp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ca8e242-2de4-4e7f-f6af-a2b2b2e4e30e"
      },
      "source": [
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1\n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(76362, 29661)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y664uBfovHkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document_lengths = pd.Series([len(x) for x in document])\n",
        "summary_lengths = pd.Series([len(x) for x in summary])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqa6ym9evHkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_maxlen = 400\n",
        "decoder_maxlen = 75"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ltH33vvHk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fFd8TZavHk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.cast(inputs, dtype=tf.int32)\n",
        "targets = tf.cast(targets, dtype=tf.int32)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYn7vwT1vHk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtwTudiKvHk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHEag_7xvHlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iNlTdlqvHlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXhMqfh9vHlO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQFs492svHlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRfeeNh_vHlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KWDfS90vHld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "            \n",
        "        return output, attention_weights"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htRz136gvHlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "    ])"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjr3EKZ0vHll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, training, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "        return out2"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkJ6mkptvHlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9FlijecvHls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxPufiD0vHlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "        return x, attention_weights"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU2h24VrvHlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USScRlf1vHl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "EPOCHS = 20"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zuBEFDevHl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MNijHGZvHl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxNoFxb0vHl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL2h3uoyvHl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbh5pTUxvHmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lizs0hm7vHmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=encoder_vocab_size, \n",
        "    pe_target=decoder_vocab_size,\n",
        ")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqOeBpJPvHmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3NasQmEvHmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')\n",
        "    "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCKS_uQUvHmM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(\n",
        "            inp, tar_inp, \n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqM4TzGHvHmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1f18705-9d5b-420a-a89a-19d5b25393ce"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):\n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        # 55k samples\n",
        "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
        "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
        "        # if batch % 429 == 0:\n",
        "        print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
        "      \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
        "    \n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
        "\n",
        "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 10.3194\n",
            "Epoch 1 Batch 1 Loss 10.3177\n",
            "Epoch 1 Batch 2 Loss 10.3172\n",
            "Epoch 1 Batch 3 Loss 10.3169\n",
            "Epoch 1 Batch 4 Loss 10.3166\n",
            "Epoch 1 Batch 5 Loss 10.3169\n",
            "Epoch 1 Batch 6 Loss 10.3161\n",
            "Epoch 1 Batch 7 Loss 10.3160\n",
            "Epoch 1 Batch 8 Loss 10.3157\n",
            "Epoch 1 Batch 9 Loss 10.3153\n",
            "Epoch 1 Batch 10 Loss 10.3147\n",
            "Epoch 1 Batch 11 Loss 10.3145\n",
            "Epoch 1 Batch 12 Loss 10.3137\n",
            "Epoch 1 Batch 13 Loss 10.3133\n",
            "Epoch 1 Batch 14 Loss 10.3127\n",
            "Epoch 1 Batch 15 Loss 10.3126\n",
            "Epoch 1 Batch 16 Loss 10.3119\n",
            "Epoch 1 Batch 17 Loss 10.3112\n",
            "Epoch 1 Batch 18 Loss 10.3106\n",
            "Epoch 1 Batch 19 Loss 10.3100\n",
            "Epoch 1 Batch 20 Loss 10.3095\n",
            "Epoch 1 Batch 21 Loss 10.3090\n",
            "Epoch 1 Batch 22 Loss 10.3083\n",
            "Epoch 1 Batch 23 Loss 10.3074\n",
            "Epoch 1 Batch 24 Loss 10.3066\n",
            "Epoch 1 Batch 25 Loss 10.3058\n",
            "Epoch 1 Batch 26 Loss 10.3048\n",
            "Epoch 1 Batch 27 Loss 10.3039\n",
            "Epoch 1 Batch 28 Loss 10.3030\n",
            "Epoch 1 Batch 29 Loss 10.3019\n",
            "Epoch 1 Batch 30 Loss 10.3007\n",
            "Epoch 1 Batch 31 Loss 10.2996\n",
            "Epoch 1 Batch 32 Loss 10.2983\n",
            "Epoch 1 Batch 33 Loss 10.2973\n",
            "Epoch 1 Batch 34 Loss 10.2960\n",
            "Epoch 1 Batch 35 Loss 10.2950\n",
            "Epoch 1 Batch 36 Loss 10.2939\n",
            "Epoch 1 Batch 37 Loss 10.2928\n",
            "Epoch 1 Batch 38 Loss 10.2917\n",
            "Epoch 1 Batch 39 Loss 10.2905\n",
            "Epoch 1 Batch 40 Loss 10.2892\n",
            "Epoch 1 Batch 41 Loss 10.2879\n",
            "Epoch 1 Batch 42 Loss 10.2866\n",
            "Epoch 1 Batch 43 Loss 10.2853\n",
            "Epoch 1 Batch 44 Loss 10.2839\n",
            "Epoch 1 Batch 45 Loss 10.2827\n",
            "Epoch 1 Batch 46 Loss 10.2812\n",
            "Epoch 1 Batch 47 Loss 10.2798\n",
            "Epoch 1 Batch 48 Loss 10.2784\n",
            "Epoch 1 Batch 49 Loss 10.2770\n",
            "Epoch 1 Batch 50 Loss 10.2757\n",
            "Epoch 1 Batch 51 Loss 10.2743\n",
            "Epoch 1 Batch 52 Loss 10.2727\n",
            "Epoch 1 Batch 53 Loss 10.2710\n",
            "Epoch 1 Batch 54 Loss 10.2697\n",
            "Epoch 1 Batch 55 Loss 10.2682\n",
            "Epoch 1 Batch 56 Loss 10.2667\n",
            "Epoch 1 Batch 57 Loss 10.2650\n",
            "Epoch 1 Batch 58 Loss 10.2635\n",
            "Epoch 1 Batch 59 Loss 10.2620\n",
            "Epoch 1 Batch 60 Loss 10.2604\n",
            "Epoch 1 Batch 61 Loss 10.2588\n",
            "Epoch 1 Batch 62 Loss 10.2573\n",
            "Epoch 1 Batch 63 Loss 10.2556\n",
            "Epoch 1 Batch 64 Loss 10.2542\n",
            "Epoch 1 Batch 65 Loss 10.2525\n",
            "Epoch 1 Batch 66 Loss 10.2510\n",
            "Epoch 1 Batch 67 Loss 10.2495\n",
            "Epoch 1 Batch 68 Loss 10.2479\n",
            "Epoch 1 Batch 69 Loss 10.2463\n",
            "Epoch 1 Batch 70 Loss 10.2446\n",
            "Epoch 1 Batch 71 Loss 10.2431\n",
            "Epoch 1 Batch 72 Loss 10.2415\n",
            "Epoch 1 Batch 73 Loss 10.2398\n",
            "Epoch 1 Batch 74 Loss 10.2382\n",
            "Epoch 1 Batch 75 Loss 10.2366\n",
            "Epoch 1 Batch 76 Loss 10.2350\n",
            "Epoch 1 Batch 77 Loss 10.2333\n",
            "Epoch 1 Batch 78 Loss 10.2317\n",
            "Epoch 1 Batch 79 Loss 10.2300\n",
            "Epoch 1 Batch 80 Loss 10.2284\n",
            "Epoch 1 Batch 81 Loss 10.2268\n",
            "Epoch 1 Batch 82 Loss 10.2251\n",
            "Epoch 1 Batch 83 Loss 10.2233\n",
            "Epoch 1 Batch 84 Loss 10.2216\n",
            "Epoch 1 Batch 85 Loss 10.2198\n",
            "Epoch 1 Batch 86 Loss 10.2182\n",
            "Epoch 1 Batch 87 Loss 10.2166\n",
            "Epoch 1 Batch 88 Loss 10.2148\n",
            "Epoch 1 Batch 89 Loss 10.2132\n",
            "Epoch 1 Batch 90 Loss 10.2114\n",
            "Epoch 1 Batch 91 Loss 10.2099\n",
            "Epoch 1 Batch 92 Loss 10.2082\n",
            "Epoch 1 Batch 93 Loss 10.2065\n",
            "Epoch 1 Batch 94 Loss 10.2047\n",
            "Epoch 1 Batch 95 Loss 10.2029\n",
            "Epoch 1 Batch 96 Loss 10.2011\n",
            "Epoch 1 Batch 97 Loss 10.1993\n",
            "Epoch 1 Batch 98 Loss 10.1977\n",
            "Epoch 1 Batch 99 Loss 10.1959\n",
            "Epoch 1 Batch 100 Loss 10.1942\n",
            "Epoch 1 Batch 101 Loss 10.1924\n",
            "Epoch 1 Batch 102 Loss 10.1906\n",
            "Epoch 1 Batch 103 Loss 10.1888\n",
            "Epoch 1 Batch 104 Loss 10.1870\n",
            "Epoch 1 Batch 105 Loss 10.1853\n",
            "Epoch 1 Batch 106 Loss 10.1834\n",
            "Epoch 1 Batch 107 Loss 10.1817\n",
            "Epoch 1 Batch 108 Loss 10.1798\n",
            "Epoch 1 Batch 109 Loss 10.1779\n",
            "Epoch 1 Batch 110 Loss 10.1760\n",
            "Epoch 1 Batch 111 Loss 10.1742\n",
            "Epoch 1 Batch 112 Loss 10.1724\n",
            "Epoch 1 Batch 113 Loss 10.1706\n",
            "Epoch 1 Batch 114 Loss 10.1686\n",
            "Epoch 1 Batch 115 Loss 10.1667\n",
            "Epoch 1 Batch 116 Loss 10.1649\n",
            "Epoch 1 Batch 117 Loss 10.1631\n",
            "Epoch 1 Batch 118 Loss 10.1612\n",
            "Epoch 1 Batch 119 Loss 10.1593\n",
            "Epoch 1 Batch 120 Loss 10.1574\n",
            "Epoch 1 Batch 121 Loss 10.1555\n",
            "Epoch 1 Batch 122 Loss 10.1535\n",
            "Epoch 1 Batch 123 Loss 10.1516\n",
            "Epoch 1 Batch 124 Loss 10.1497\n",
            "Epoch 1 Batch 125 Loss 10.1476\n",
            "Epoch 1 Batch 126 Loss 10.1456\n",
            "Epoch 1 Batch 127 Loss 10.1436\n",
            "Epoch 1 Batch 128 Loss 10.1416\n",
            "Epoch 1 Batch 129 Loss 10.1396\n",
            "Epoch 1 Batch 130 Loss 10.1376\n",
            "Epoch 1 Batch 131 Loss 10.1357\n",
            "Epoch 1 Batch 132 Loss 10.1335\n",
            "Epoch 1 Batch 133 Loss 10.1314\n",
            "Epoch 1 Batch 134 Loss 10.1293\n",
            "Epoch 1 Batch 135 Loss 10.1273\n",
            "Epoch 1 Batch 136 Loss 10.1252\n",
            "Epoch 1 Batch 137 Loss 10.1231\n",
            "Epoch 1 Batch 138 Loss 10.1210\n",
            "Epoch 1 Batch 139 Loss 10.1189\n",
            "Epoch 1 Batch 140 Loss 10.1168\n",
            "Epoch 1 Batch 141 Loss 10.1146\n",
            "Epoch 1 Batch 142 Loss 10.1124\n",
            "Epoch 1 Batch 143 Loss 10.1104\n",
            "Epoch 1 Batch 144 Loss 10.1083\n",
            "Epoch 1 Batch 145 Loss 10.1059\n",
            "Epoch 1 Batch 146 Loss 10.1038\n",
            "Epoch 1 Batch 147 Loss 10.1016\n",
            "Epoch 1 Batch 148 Loss 10.0993\n",
            "Epoch 1 Batch 149 Loss 10.0970\n",
            "Epoch 1 Batch 150 Loss 10.0949\n",
            "Epoch 1 Batch 151 Loss 10.0927\n",
            "Epoch 1 Batch 152 Loss 10.0904\n",
            "Epoch 1 Batch 153 Loss 10.0881\n",
            "Epoch 1 Batch 154 Loss 10.0858\n",
            "Epoch 1 Batch 155 Loss 10.0835\n",
            "Epoch 1 Batch 156 Loss 10.0811\n",
            "Epoch 1 Batch 157 Loss 10.0786\n",
            "Epoch 1 Batch 158 Loss 10.0763\n",
            "Epoch 1 Batch 159 Loss 10.0741\n",
            "Epoch 1 Batch 160 Loss 10.0718\n",
            "Epoch 1 Batch 161 Loss 10.0694\n",
            "Epoch 1 Batch 162 Loss 10.0670\n",
            "Epoch 1 Batch 163 Loss 10.0646\n",
            "Epoch 1 Batch 164 Loss 10.0621\n",
            "Epoch 1 Batch 165 Loss 10.0597\n",
            "Epoch 1 Batch 166 Loss 10.0574\n",
            "Epoch 1 Batch 167 Loss 10.0552\n",
            "Epoch 1 Batch 168 Loss 10.0528\n",
            "Epoch 1 Batch 169 Loss 10.0503\n",
            "Epoch 1 Batch 170 Loss 10.0480\n",
            "Epoch 1 Batch 171 Loss 10.0455\n",
            "Epoch 1 Batch 172 Loss 10.0430\n",
            "Epoch 1 Batch 173 Loss 10.0405\n",
            "Epoch 1 Batch 174 Loss 10.0380\n",
            "Epoch 1 Batch 175 Loss 10.0356\n",
            "Epoch 1 Batch 176 Loss 10.0330\n",
            "Epoch 1 Batch 177 Loss 10.0306\n",
            "Epoch 1 Batch 178 Loss 10.0282\n",
            "Epoch 1 Batch 179 Loss 10.0255\n",
            "Epoch 1 Batch 180 Loss 10.0228\n",
            "Epoch 1 Batch 181 Loss 10.0203\n",
            "Epoch 1 Batch 182 Loss 10.0177\n",
            "Epoch 1 Batch 183 Loss 10.0150\n",
            "Epoch 1 Batch 184 Loss 10.0125\n",
            "Epoch 1 Batch 185 Loss 10.0100\n",
            "Epoch 1 Batch 186 Loss 10.0073\n",
            "Epoch 1 Batch 187 Loss 10.0047\n",
            "Epoch 1 Batch 188 Loss 10.0022\n",
            "Epoch 1 Batch 189 Loss 9.9996\n",
            "Epoch 1 Batch 190 Loss 9.9968\n",
            "Epoch 1 Batch 191 Loss 9.9940\n",
            "Epoch 1 Batch 192 Loss 9.9913\n",
            "Epoch 1 Batch 193 Loss 9.9886\n",
            "Epoch 1 Batch 194 Loss 9.9859\n",
            "Epoch 1 Batch 195 Loss 9.9833\n",
            "Epoch 1 Batch 196 Loss 9.9806\n",
            "Epoch 1 Batch 197 Loss 9.9779\n",
            "Epoch 1 Batch 198 Loss 9.9751\n",
            "Epoch 1 Batch 199 Loss 9.9724\n",
            "Epoch 1 Batch 200 Loss 9.9699\n",
            "Epoch 1 Batch 201 Loss 9.9670\n",
            "Epoch 1 Batch 202 Loss 9.9642\n",
            "Epoch 1 Batch 203 Loss 9.9613\n",
            "Epoch 1 Batch 204 Loss 9.9585\n",
            "Epoch 1 Batch 205 Loss 9.9557\n",
            "Epoch 1 Batch 206 Loss 9.9529\n",
            "Epoch 1 Batch 207 Loss 9.9500\n",
            "Epoch 1 Batch 208 Loss 9.9471\n",
            "Epoch 1 Batch 209 Loss 9.9443\n",
            "Epoch 1 Batch 210 Loss 9.9414\n",
            "Epoch 1 Batch 211 Loss 9.9387\n",
            "Epoch 1 Batch 212 Loss 9.9358\n",
            "Epoch 1 Batch 213 Loss 9.9329\n",
            "Epoch 1 Batch 214 Loss 9.9301\n",
            "Epoch 1 Batch 215 Loss 9.9273\n",
            "Epoch 1 Batch 216 Loss 9.9244\n",
            "Epoch 1 Batch 217 Loss 9.9216\n",
            "Epoch 1 Batch 218 Loss 9.9187\n",
            "Epoch 1 Batch 219 Loss 9.9157\n",
            "Epoch 1 Batch 220 Loss 9.9128\n",
            "Epoch 1 Batch 221 Loss 9.9100\n",
            "Epoch 1 Batch 222 Loss 9.9071\n",
            "Epoch 1 Batch 223 Loss 9.9041\n",
            "Epoch 1 Batch 224 Loss 9.9012\n",
            "Epoch 1 Batch 225 Loss 9.8982\n",
            "Epoch 1 Batch 226 Loss 9.8951\n",
            "Epoch 1 Batch 227 Loss 9.8920\n",
            "Epoch 1 Batch 228 Loss 9.8889\n",
            "Epoch 1 Batch 229 Loss 9.8860\n",
            "Epoch 1 Batch 230 Loss 9.8829\n",
            "Epoch 1 Batch 231 Loss 9.8798\n",
            "Epoch 1 Batch 232 Loss 9.8769\n",
            "Epoch 1 Batch 233 Loss 9.8738\n",
            "Epoch 1 Batch 234 Loss 9.8708\n",
            "Epoch 1 Batch 235 Loss 9.8679\n",
            "Epoch 1 Batch 236 Loss 9.8648\n",
            "Epoch 1 Batch 237 Loss 9.8616\n",
            "Epoch 1 Batch 238 Loss 9.8584\n",
            "Epoch 1 Batch 239 Loss 9.8553\n",
            "Epoch 1 Batch 240 Loss 9.8522\n",
            "Epoch 1 Batch 241 Loss 9.8492\n",
            "Epoch 1 Batch 242 Loss 9.8460\n",
            "Epoch 1 Batch 243 Loss 9.8428\n",
            "Epoch 1 Batch 244 Loss 9.8399\n",
            "Epoch 1 Batch 245 Loss 9.8366\n",
            "Epoch 1 Batch 246 Loss 9.8334\n",
            "Epoch 1 Batch 247 Loss 9.8300\n",
            "Epoch 1 Batch 248 Loss 9.8269\n",
            "Epoch 1 Batch 249 Loss 9.8238\n",
            "Epoch 1 Batch 250 Loss 9.8207\n",
            "Epoch 1 Batch 251 Loss 9.8175\n",
            "Epoch 1 Batch 252 Loss 9.8143\n",
            "Epoch 1 Batch 253 Loss 9.8113\n",
            "Epoch 1 Batch 254 Loss 9.8080\n",
            "Epoch 1 Batch 255 Loss 9.8046\n",
            "Epoch 1 Batch 256 Loss 9.8013\n",
            "Epoch 1 Batch 257 Loss 9.7980\n",
            "Epoch 1 Batch 258 Loss 9.7948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As5wM_LCvHmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#FOR INFERENCE\n",
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
        "\n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(decoder_maxlen):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        predictions, attention_weights = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n",
        "            return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0), attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmoEiEE2vHmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize(input_document):\n",
        "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
        "    summarized = evaluate(input_document=input_document)[0].numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2KDyGs8vHma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summarize(\"Nitish Kumar-led Bihar government has reduced the hike in power tariff to 28% after an earlier 55% hike by Bihar Electricity Regulatory Commission (BERC) sparked protests by the Opposition. The hike will be limited to 28% after the government announced that it will continue the subsidy in the power sector which would go directly into the bank accounts of consumers.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}